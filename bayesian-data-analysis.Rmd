---
title: "Bayesian Data Analysis"
author: "Alexandre Galiani Garmbis"
date: "29/07/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(tidyverse)
library(ggridges)
library(lattice)
library(rjags)
library(BEST)
library(openintro)
library(mosaic)
```

## Análise de dados Bayesiana

Teorema de Bayes

$$P(\theta|D)=\frac{P(D|\theta) \times P(\theta)}{\sum P(D|\theta) \times P(\theta)}$$
**Interpretação:** A probabilidade de diferentes valores para determinados parametros dado algumas observações experimentais ($P(\theta|D)$) é igual a probabilidade relativa (verossimilhança) dessas observaçoes dados diferentes valores para estes parametros ($P(D|\theta)$) vezes a probabilidade de diferentes parametros ($P(\theta)$) antes de observar os dados (conhecimento a priori) dividido pela soma da verossimilhança ponderada pelo probabilidade a priori ($\sum P(D|\theta) \times P(\theta)$).

```{r, echo=FALSE}
prop_model <- function(data = c(), prior_prop = c(1, 1), n_draws = 10000) {
  data <- as.logical(data)
  proportion_success <- c(0, seq(0, 1, length.out = 100), 1)
  data_indices <- round(seq(0, length(data), length.out = min(length(data) + 1, 20)))

  post_curves <- map_dfr(data_indices, function(i) {
    value <- ifelse(i == 0, "Prior", ifelse(data[i], "Success", "Failure"))
    label <- paste0("n=", i)
    probability <- dbeta(proportion_success, prior_prop[1] + sum(data[seq_len(i)]), prior_prop[2] + sum(!data[seq_len(i)]))
    probability <- probability / max(probability)
    data_frame(value, label, proportion_success, probability)
  })
  post_curves$label <- fct_rev(factor(post_curves$label, levels =  paste0("n=", data_indices )))
  post_curves$value <- factor(post_curves$value, levels = c("Prior", "Success", "Failure"))
  
  p <- ggplot(post_curves, aes(x = proportion_success, y = label, height = probability, fill = value)) +
    geom_density_ridges(stat="identity", color = "white", alpha = 0.8, panel_scaling = TRUE, size = 1) +
    scale_y_discrete("", expand = c(0.01, 0)) +
    scale_x_continuous("Underlying proportion of success") +
    scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), name = "", drop = FALSE, labels =  c("Prior   ", "Success   ", "Failure   ")) +
    #ggtitle(paste0("Binomial model - Data: ", sum(data),  " successes, " , sum(!data), " failures"))  +
    theme_light(base_size = 18) +
    theme(legend.position = "top")
  print(p)
  
  invisible(rbeta(n_draws, prior_prop[1] + sum(data), prior_prop[2] + sum(!data)))
}

data <- sample(c(1,0), size=100, replace = TRUE, prob = c(0.15,0.85))
posterior <- prop_model(data)
```

## Grid approximation

O problema com a solução anterior é que ela não escala muito bem.

```{r}
# The IQ of a bunch of zombies
iq <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)
# Defining the parameter grid
pars <- expand.grid(mu = seq(0, 150, length.out = 100), 
                    sigma = seq(0.1, 50, length.out = 100))
# Defining and calculating the prior density for each parameter combination
pars$mu_prior <- dnorm(pars$mu, mean = 100, sd = 100)
pars$sigma_prior <- dunif(pars$sigma, min = 0.1, max = 50)
pars$prior <- pars$mu_prior * pars$sigma_prior
# Calculating the likelihood for each parameter combination
for(i in 1:nrow(pars)) {
  likelihoods <- dnorm(iq, pars$mu[i], pars$sigma[i])
  pars$likelihood[i] <- prod(likelihoods)
}
# Calculate the probability of each parameter combination
pars$probability <- pars$likelihood * pars$prior/
  sum(pars$likelihood * pars$prior)

head(pars)
```

```{r}
levelplot(probability ~ mu * sigma, data = pars)
```

### Intevalo de confiança com bootstrap
```{r}
# Sample from pars to calculate some new measures
sample_indices <- sample( nrow(pars), size = 10000,
    replace = TRUE, prob = pars$probability)
pars_sample <- pars[sample_indices, c("mu","sigma")]
quantile(pars_sample$mu, c(0.025,0.5,0.975))
```

# BEST - Bayesian Estimation Supersedes the t Test

BEST é um pacote que utiliza o JAGS para especificar o algoritmo MCMC (Markoc Chain Monte Carlo).

```{r}
iq_brains <- c(44, 52, 42, 66, 53, 42, 55, 57, 56, 51)
iq_regular <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)

fit <- BESTmcmc(iq_brains, iq_regular)
fit
```
```{r}
plot(fit)
```

# JAGS - Just Another Gibbs Sampler

O pacote rjags faz a interface do R com o programa JAGS.

## Modelo Beta-Binomial

O primeiro passo é definir o modelo:
```{r}


# DEFINE the model
vote_model <- "model{
    # Likelihood model for X
    X ~ dbin(p, n)
    
    # Prior model for p
    p ~ dbeta(a, b)
}"
```

O segundo passo é compilar um algoritmo para estimar a distribuição a posteriori:
```{r}
# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 45, b = 55, X = 6, n = 10),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))
```

O último passo é simular a distribuição a posteriori:
```{r}
# SIMULATE the posterior
vote_sim0 <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)
```

### Visualização
A funçao coda.samples gera uma lista com atributos para visualizar o resultado da simulação
```{r}
# PLOT the posterior
plot(vote_sim0, trace = FALSE)
```

### Efeito da mudança nas premissas do modelo

Alterando a distribuição a priori para Beta(1,1)
```{r, message=FALSE}
# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 1, b = 1, X = 6, n = 10),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim1 <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)
```

Adicionalmente, uma nova pesquisa indica que 214 de 390 entrevistados votariam no candidato. Somado aos 10 anteriores, são 220 de 400.
```{r}
# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 1, b = 1, X = 220, n = 400),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim2 <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)
```

Considerando a distribuição a priori utilizada anteriormente Beta(45,55)
```{r}
# COMPILE the model    
vote_jags <- jags.model(textConnection(vote_model), 
    data = list(a = 45, b = 55, X = 220, n = 400),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
vote_sim3 <- coda.samples(model = vote_jags, variable.names = c("p"), n.iter = 10000)
```

### Visualização
```{r}
prior <- rbeta(10000, 45, 55)
noprior <- rbeta(10000, 1, 1)

sims <- map(list(prior,noprior,vote_sim0,vote_sim1,vote_sim2,vote_sim3),unlist) %>% 
  set_names(c("prior-nodata","noprior-nodata","prior-fewdata",
              "noprior-fewdata","noprior-moredata","prior-moredata")) %>%
  bind_rows() %>% gather("sim","votes") %>% separate(sim, into=c("prior","data"))

ggplot(sims, aes(votes, color=prior)) + geom_density() +
  facet_wrap(~data, ncol=1)
```

## Modelo Normal-Normal

```{r}
sleep_study <- read_csv("data/sleep_study.csv") %>% select(-id)
# DEFINE the model
sleep_model <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
        Y[i] ~ dnorm(m, s^(-2))
    }

    # Prior models for m and s
    m ~ dnorm(50, 25^(-2))
    s ~ dunif(0, 200)
}"  

# COMPILE the model    
sleep_jags <- jags.model(textConnection(sleep_model), 
    data = list(Y = sleep_study$diff_3),
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))

# SIMULATE the posterior
sleep_sim <- coda.samples(model = sleep_jags, variable.names = c("m","s"), n.iter = 10000)
plot(sleep_sim, trace = TRUE, density = TRUE)
```

```{r}
# Store the chains in a data frame
sleep_chains <- data.frame(sleep_sim[[1]], iter = 1:10000)

# Check out the head of sleep_chains
head(sleep_chains)
```

```{r}
# Trace plot the first 100 iterations of the m chain
ggplot(head(sleep_chains, 100), aes(x = iter, y = m)) + 
    geom_line()
```

```{r}
# COMPILE the model
sleep_jags_multi <- jags.model(textConnection(sleep_model), 
  data = list(Y = sleep_study$diff_3),
  n.chains = 4 )   

# SIMULATE the posterior    
sleep_sim_multi <- coda.samples(model = sleep_jags_multi, 
  variable.names = c("m", "s"), n.iter = 1000)

# Construct trace plots of the m and s chains
plot(sleep_sim_multi, density = FALSE)
```
```{r}
summary(sleep_sim_multi)
```

## Regressão Linear

### Priors
```{r}
# Samples
n <- 10000
samples <- data.frame(prior_a = rnorm(n, 0, 200),
                      prior_b = rnorm(n, 1, 0.5),
                      prior_s = runif(n, 0, 20),
                      set = 1:10000)

# Replicate the first 12 parameter sets 50 times each
prior_scenarios_rep <- bind_rows(replicate(n = 50, 
  expr = head(samples,12), simplify = FALSE)) 

# Simulate 50 height & weight data points for each parameter set
prior_simulation <- prior_scenarios_rep %>% 
    mutate(height = rnorm(600, 170, 10)) %>% 
    mutate(weight = rnorm(600, prior_a + prior_b*height, prior_s))

# Plot the simulated data & regression model for each parameter set
ggplot(prior_simulation, aes(x = height, y = weight)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE, size = 0.75) + 
    facet_wrap(~ set)
```

### Dados

```{r}
# From OpenIntro
ggplot(bdims, aes(x = hgt, y = wgt)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE)
```

### Regressão apenas com os dados

```{r}
# Obtain the sample regression model
wt_model <- lm(wgt ~ hgt, data = bdims)

# Summarize the model
summary(wt_model)
```

### Regressão bayesiana

```{r}
# DEFINE the model
weight_model <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)) {
        Y[i] ~ dnorm(m[i], s^(-2))
        m[i] = a + b*X[i]
    }

    # Prior models for m and s
    a ~ dnorm(0, 200^(-2))
    b ~ dnorm(1, 0.5^(-2))
    s ~ dunif(0, 20)
}"  

# COMPILE the model    
weight_jags <- jags.model(textConnection(weight_model), 
    data = list(Y = bdims$wgt, X = bdims$hgt),
    n.chains = 4)

# SIMULATE the posterior
weight_sim <- coda.samples(model = weight_jags, variable.names = c("a","b","s"), n.iter = 1000)
plot(weight_sim, trace = TRUE, density = TRUE)
```

O modelo não convergiu bem. As variáveis independentes a e b não estabilizaram e aparentam ter uma correlação negativa. De certa forma o resutado é esperado: mudando a inclinaçao da reta, a interseção muda também. Variáveis correlacionadas requerem um maior número de simulações. Para melhorar tal instabilidade, também é possível padronizar os dados de entrada (z). Nesse caso, vamos pelo caminho mais fácil: simplesmente aumentar o número de amostras.

```{r}
# COMPILE the model    
weight_jags <- jags.model(textConnection(weight_model), 
    data = list(Y = bdims$wgt, X = bdims$hgt),
    n.chains = 1)

# SIMULATE the posterior
weight_sim <- coda.samples(model = weight_jags, variable.names = c("a","b","s"), n.iter = 100000)
plot(weight_sim, trace = TRUE, density = TRUE)
```
Agora sim. ;-)

### Visualizando os resultados
```{r}
n <- 100000
prior <- data_frame(a = rnorm(n, 0, 200),
                    b = rnorm(n, 1, 0.5),
                    s = runif(n, 0, 20))
posterior <- data.frame(weight_sim[[1]]) %>% as_data_frame()
bayes <- bind_rows(prior = prior, posterior = posterior, .id = "dist") %>% gather("var","value",-dist)
ggplot(bayes, aes(x=value, color=dist)) + geom_density() + facet_wrap(~var, scales="free") +
  theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank())
```

### Comparação dos coeficientes 

```{r}
summary(weight_sim)[1]$statistics[,1:2] #regressão bayesiana
```
```{r}
summary(wt_model)$coefficients[,1:2] #regressão linear
```

```{r}
ggplot(bdims, aes(hgt,wgt)) + geom_point() + geom_smooth(method = "lm") +
  geom_abline(data=sample_n(posterior,200), aes(slope= b, intercept = a), alpha=0.01, color="red")
```

Praticamente a mesma coisa. Funciona pelo menos :-)

### Intervalos de confiança
```{r}
summary(weight_sim)[2]$quantiles
```
```{r}
ci_90 <- quantile(posterior$b, c(0.025,0.975))
ci_90
```

```{r}
ggplot(posterior, aes(x = b)) + 
    geom_density() + 
    geom_vline(xintercept = ci_90, color = "red")
```

### Probabilidades com a distribuição a posteriori

```{r}
# Calculate the proportion of b chain values that exceed 1.1 
mean(posterior$b > 1.1)
```

### Análises preditivas com a distribuição a posteriori

```{r}
pred <- posterior %>% mutate(m_174 = a + b * 174, Y_174 = rnorm(n(), m_174, s))
pred
```

Peso médio para um intervalo de confiança de 95%:
```{r}
(ci_m_174 <- quantile(pred$m_174, c(0.025,0.975)))
```

Peso para um intervalo de confiança de 95%:
```{r}
(ci_Y_174 <- quantile(pred$Y_174, c(0.025,0.975)))
```
```{r}
ggplot(bdims, aes(x = hgt, y = wgt)) + 
    geom_point() + 
    geom_abline(intercept = mean(posterior$a), slope = mean(posterior$b), color = "red") + 
    geom_segment(x = 174, xend = 174, y = ci_Y_174[1], yend = ci_Y_174[2], color = "red")
```

## Regressão bayesiana com dados categóricos

### Dados
```{r}
# From Mosaic
data("RailTrail")
as_tibble(RailTrail)
```
```{r}
# Construct a sample model
rail_lm <- lm(volume ~ weekday + hightemp, data = RailTrail)

# Extract the model coefficients
rail_coefs <- coef(rail_lm)

# Superimpose sample estimates of the model lines

ggplot(RailTrail, aes(x=hightemp, y=volume, color=weekday)) + 
    geom_point() + 
    geom_abline(intercept = rail_coefs[1], slope = rail_coefs[3], color = "red") +
    geom_abline(intercept = rail_coefs[1] + rail_coefs[2], slope = rail_coefs[3], color = "turquoise3")
```

### Modelo
```{r}

RailTrail$weekday <- factor(RailTrail$weekday)
RailTrail$weekday <- droplevels(RailTrail$weekday)
RailTrail$weekday <- as.integer(RailTrail$weekday)


# DEFINE the model    
rail_model_2 <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)){
        Y[i] ~ dnorm(m[i], s^(-2))
        m[i] = a + b[X[i]] + c * Z[i]
    }
    
    # Prior models for a, b, s
    a    ~ dnorm(0, 200^(-2))
    b[1] = 0
    b[2] ~ dnorm(0, 200^(-2))
    c    ~ dnorm(0, 20^(-2))
    s    ~ dunif(0, 200)
}"   

# COMPILE the model
rail_jags_2 <- jags.model(textConnection(rail_model_2), 
    data = list(Y = RailTrail$volume, X = RailTrail$weekday, Z = RailTrail$hightemp), 
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10))

# SIMULATE the posterior    
rail_sim_2 <- coda.samples(model = rail_jags_2,
                  variable.names = c("a", "b", "c", "s"),
                  n.iter = 10000)

# Store the chains in a data frame
rail_chains_2 <- data.frame(rail_sim_2[[1]])

# PLOT the posterior    
plot(rail_sim_2)
```
### Coeficientes
```{r}
summary(rail_sim_2)
```

### Inferência
```{r}
# Posterior probability that typical volume is lower on weekdays
mean(rail_chains_2$b.2. < 0)
```
```{r}
# Construct a chain of values for the typical weekday volume
rail_chains_2 <- rail_chains_2 %>% 
    mutate(weekday_mean = a + b.2.)

# 95% credible interval for typical weekday volume
ci_95 <- quantile(rail_chains_2$weekday_mean, c(0.025,0.975))

# Construct a density plot of the weekday chain
ggplot(rail_chains_2, aes(x=weekday_mean)) + geom_density() +
  geom_vline(xintercept = ci_95, color = "red")
```

### Visualização
```{r}
# Plot the posterior mean regression models
ggplot(RailTrail, aes(x = hightemp, y = volume)) + 
    geom_point() + 
    geom_abline(intercept = mean(rail_chains_2$a), slope = mean(rail_chains_2$c), color = "red") + 
    geom_abline(intercept = mean(rail_chains_2$a) + mean(rail_chains_2$b.2.), slope = mean(rail_chains_2$c), color = "turquoise3")
```

## Regressão bayesiana e variáveis discretas (Poisson)

O modelo que considera o volume como um processo estocástico normal tem dois problemas: a distribuição preve volumes menor do que zero, mesmo que improváveis, e continuos, como por exemplo uma pessoa e meia. O processo de Poisson pode corrigir esses dois problemas, mas introduz outro: ele considera que a variância no volume é igual ao volume médio. Nossos dados apresentam uma variância muito maior.

### Modelo
```{r}
# DEFINE the model    
rail_model_3 <- "model{
    # Likelihood model for Y[i]
    for(i in 1:length(Y)){
        Y[i] ~ dpois(l[i])
        log(l[i]) = a + b[X[i]] + c * Z[i]
    }
    
    # Prior models for a, b, c
    a    ~ dnorm(0, 200^(-2))
    b[1] = 0
    b[2] ~ dnorm(0, 2^(-2))
    c    ~ dnorm(0, 2^(-2))
}"   

# COMPILE the model
rail_jags_3 <- jags.model(textConnection(rail_model_3), 
    data = list(Y = RailTrail$volume, X = RailTrail$weekday, Z = RailTrail$hightemp), 
    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 10))

# SIMULATE the posterior    
rail_sim_3 <- coda.samples(model = rail_jags_3,
                  variable.names = c("a", "b", "c", "s"),
                  n.iter = 10000)

# Store the chains in a data frame
rail_chains_3 <- data.frame(rail_sim_3[[1]])

# PLOT the posterior    
plot(rail_sim_3)
```

### Coeficientes
```{r}
# Summarize the posterior Markov chains
summary(rail_sim_3)
```

### Visualização
```{r}
# Plot the posterior mean regression models
coeff <- summary(rail_sim_3)$statistics[,1]
ggplot(RailTrail, aes(x = hightemp, y = volume, color = weekday)) + 
    geom_point() + 
    stat_function(fun = function(x){exp(coeff['a'] + coeff['c'] * x)}, color = "red") + 
    stat_function(fun = function(x){exp(coeff['a'] + coeff['b[2]'] + coeff['c'] * x)}, color = "turquoise3")
```

### Intervalos de confiança
```{r}
# Calculate the typical volume on 80 degree weekends & 80 degree weekdays
poisson_chains <- rail_chains_3 %>% 
    mutate(l_weekend = exp(a + b.1. + c * 80),
           l_weekday = exp(a + b.2. + c * 80),
           Y_weekend = rpois(n(), l_weekend),
           Y_weekday = rpois(n(), l_weekday))

# Construct a 95% CI for typical volume on 80 degree weekend
quantile(poisson_chains$l_weekend, c(0.025, 0.975))

# Construct a 95% CI for typical volume on 80 degree weekday
quantile(poisson_chains$l_weekday, c(0.025, 0.975))
```

### Inferência
```{r}
# Posterior probability that weekday volume is less 400
prob_less_400 <- mean(poisson_chains$Y_weekday < 400)

# Construct a density plot of the posterior weekday predictions
poisson_chains %>% select(Y_weekend,Y_weekday) %>% 
  gather("day","volume") %>% separate(day, into = c("var","day")) %>%
  ggplot(aes(x=volume)) + geom_density(aes(fill=day), alpha=0.5) +
  geom_vline(xintercept = 400, color = "red") +
  annotate(geom = "text", x=385, y=0.002, label=paste0(prob_less_400*100,"%"))
```


